{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Student.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Eminent01/AMMI-WORK/blob/main/NLP_Student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Az99b8EBkjBl"
      },
      "source": [
        "# Gensim: a package to train and use word vectors\n",
        "\n",
        "Gensim is a Python package that allows to train and use word vectors.\n",
        "A lot of functions for word vectors analysis are implemented by Gensim."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7JcTMktkpm5"
      },
      "source": [
        "import gensim\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXIPJcnokv4X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a7784ab9-8f47-46be-b260-b51133c89947"
      },
      "source": [
        "import gensim.downloader as api\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", \"Conversion of the second argument of issubdtype from\", FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", \"This function is deprecated, use smart_open.open instead. See the migration notes for \", UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", \"arrays to stack must be passed as a\", FutureWarning)\n",
        "\n",
        "# This will load word vectors for a large vocabulary \n",
        "# It should take < 5 minutes\n",
        "wv = api.load('glove-wiki-gigaword-50')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sIZatF3ldF4"
      },
      "source": [
        "In the following cell, we download a list of analogies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3EPq3ol10nh",
        "outputId": "d5a88682-3128-4fdc-9506-69513a01973a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/nicholas-leonard/word2vec/master/questions-words.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-02 18:30:46--  https://raw.githubusercontent.com/nicholas-leonard/word2vec/master/questions-words.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 603955 (590K) [text/plain]\n",
            "Saving to: ‘questions-words.txt’\n",
            "\n",
            "\rquestions-words.txt   0%[                    ]       0  --.-KB/s               \rquestions-words.txt 100%[===================>] 589.80K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2019-12-02 18:30:46 (15.5 MB/s) - ‘questions-words.txt’ saved [603955/603955]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGHfli3bvpmB"
      },
      "source": [
        "In the next cell, open the downloaded file and take a look to see what it contains.\n",
        "Print the first 5 lines."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAe8dm4HvsXF",
        "outputId": "f86e1652-3212-4f02-ba81-99c6a65d16d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "f = open('questions-words.txt', 'r')\n",
        "for line in f.readlines()[:5]:\n",
        "    print(line)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ": capital-common-countries\n",
            "\n",
            "Athens Greece Baghdad Iraq\n",
            "\n",
            "Athens Greece Bangkok Thailand\n",
            "\n",
            "Athens Greece Beijing China\n",
            "\n",
            "Athens Greece Berlin Germany\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWMZfPBevv8q"
      },
      "source": [
        "Note that :\n",
        "- Athens is to Greece what Baghdad is to Iraq\n",
        "- Athens is to Greece what Bangkok is to Thailand\n",
        "- Athens is to Greece what Beijing is to China\n",
        "etc...\n",
        "\n",
        "Remember the word analogy task : \n",
        "A is to B what C is to D\n",
        "\n",
        "As we saw before, using word vectors, we can try to guess D using the word vectors of A, B and C !\n",
        "\n",
        "We use the formula :\n",
        "$$B-A + C \\approx D$$\n",
        "\n",
        "For each analogy (A,B,C,D) described in the file \"questions-words.txt\", we can try to :\n",
        "- compute $\\tilde{C} = B-A + C$ with the word vectors\n",
        "- compute the nearest neighbors of $\\tilde{C}$ in the word vectors.\n",
        "- If the nearest neighbor is $C$, we've answered correctly to the analogy, otherwise we didn't.\n",
        "\n",
        "Averaging correct / incorrect answers on each analogies gives us a metric. This metric can be used as an indicator of quality of the word embeddings.\n",
        "\n",
        "In the next cell, we will ask Gensim to compute this metric for us."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrZCcs4b1a5I",
        "outputId": "e86aca21-5e0a-4aad-d24a-61362849ac06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# This should take approximately 5 minutes\n",
        "\n",
        "start = time.time()\n",
        "results = wv.evaluate_word_analogies(analogies='questions-words.txt')\n",
        "print(f\"Accuracy on the world analogy task {results[0]}\")\n",
        "print(f\"Evaluating this took {time.time() - start}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on the world analogy task 0.463717540798522\n",
            "Evaluating this took 276.7814600467682\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xao9cuCwJoH4"
      },
      "source": [
        "### Similar words\n",
        "\n",
        "Each word is associated with an index value (between 0 and $n_{words} - 1$).\n",
        "If two words have similar spelling, this information will be lost as the words are replaced with their indices.\n",
        "However, we can see that the model still discovers similar meanings automatically: if I take the two words \"car\" and \"cars\", the model has put these vectors close together. \n",
        "\n",
        "Can you guess why ?\n",
        "\n",
        "We can use gensim to check the closest word to a word.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viS4Ecmrk9xQ",
        "outputId": "f51c6427-88ac-46a6-f9ac-63d2bd5d39a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(wv.most_similar(positive=['car'], topn=5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('truck', 0.9208585619926453), ('cars', 0.8870190382003784), ('vehicle', 0.8833684325218201), ('driver', 0.8464018702507019), ('driving', 0.8384189009666443)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDhX8_Jcv5_O"
      },
      "source": [
        "Question: What does this do ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CP_VckvXliMp",
        "outputId": "891a051d-329b-4eab-f6b2-b934e8d9dd6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(wv.doesnt_match(['fire', 'water', 'land', 'sea', 'air', 'car']))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "car\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4RPeMlMlsKe"
      },
      "source": [
        "## Train your own word vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDd7yHJ5nDIi"
      },
      "source": [
        "Download and extract the IMDB Sentiment classification corpus\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVHVQKtanCHn",
        "outputId": "9ba55638-c021-4aac-a296-12fa3329cefe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar xzf aclImdb_v1.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-02 18:35:28--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz’\n",
            "\n",
            "aclImdb_v1.tar.gz   100%[===================>]  80.23M  67.6MB/s    in 1.2s    \n",
            "\n",
            "2019-12-02 18:35:29 (67.6 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldHMD6ghwBcD"
      },
      "source": [
        "In the following cell, we define a class called GensimCorpus. Can you guess what it does ? The code look complicated ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFBGqopKl1f5"
      },
      "source": [
        "from gensim import utils\n",
        "import os\n",
        "from os.path import join\n",
        "\n",
        "class GensimCorpus(object):\n",
        "    \"\"\"An interator that yields sentences (lists of str).\"\"\"\n",
        "\n",
        "    def __init__(self, path=\"aclImdb/train/unsup\"):\n",
        "        self.path = path\n",
        "        self.filenames = [fname for fname in os.listdir(path) if fname.endswith(\".txt\")]\n",
        "        \n",
        "\n",
        "    def __iter__(self):\n",
        "        for fname in self.filenames:\n",
        "            # assume there's one document per line, tokens separated by whitespace\n",
        "            with open(join(self.path, fname), \"r\") as f:\n",
        "                lines = f.readlines()\n",
        "                assert len(lines) == 1\n",
        "                line = lines[0].strip()\n",
        "                yield utils.simple_preprocess(line)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1J3mve-wD3T"
      },
      "source": [
        "In  order to understand what it does, we will try to use it. ```__iter__ ``` is defined for generators, let's try to see what this generator outputs !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYBJbTQnrHpr"
      },
      "source": [
        "sentences = GensimCorpus()\n",
        "\n",
        "#TODO: look at a few examples "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-APUBdAwIfS"
      },
      "source": [
        "Gensim can learn word embeddings ( == word vectors) from a text corpus. Let's learn word embeddings on our GensimCorpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvYkDKq5lvEh",
        "outputId": "6c03fbc2-d420-4e97-ed9f-7777254a4bba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import gensim.models\n",
        "\n",
        "# This should take approximately 3 minutes\n",
        "start = time.time()\n",
        "model = gensim.models.Word2Vec(sentences=sentences, size=50)\n",
        "print(\"Took %.2f\" % (time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Took 150.86\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jC-Ax6hwK7S"
      },
      "source": [
        "The model we learned, ```model``` contains word vectors ```model.wv```. We can use ```evaluate_word_analogies``` as before to evaluate the quality of our newly learned word vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWlSZg4jtDdN"
      },
      "source": [
        "#TODO: evaluate word analogies "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxnU5YmBwOd8"
      },
      "source": [
        "What about nearest neighbors ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LWRKJ7ut7Xt"
      },
      "source": [
        "#TODO: look at nearest neighbors of common words such as king, queen, etc. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmKXkGDgVlaN"
      },
      "source": [
        "Question: Is this model as accurate as the previous one ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khgKL5tv3_WH",
        "outputId": "d85f4190-15cc-48e4-84df-e797ef2cfdfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(model.wv.doesnt_match(['fire', 'water', 'land', 'sea', 'air', 'car']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "car\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75I9wu3mKrj9"
      },
      "source": [
        "## Text classification with the bag of words model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRqWJfq_45lA"
      },
      "source": [
        "We have trained our own word vector model on data from IMDB. \n",
        "We now want to perform sentiment analysis: predict whether a review is positive or negative. \n",
        "First, start by creating a dataset with the positive and negative sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTuicK6cC_Sl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "bfb7243d-4b4b-428e-a137-bedbc1db35ab"
      },
      "source": [
        "positives = GensimCorpus(path=...)\n",
        "negatives = GensimCorpus(path=...)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-bf95750c3259>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpositives\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGensimCorpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnegatives\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGensimCorpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-680e8503a517>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"aclImdb/train/unsup\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: listdir: path should be string, bytes, os.PathLike, integer or None, not ellipsis"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERqxW8GbHPmG"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def create_embeddings(corpus, word_vectors):\n",
        "  # TODO: create a function that takes a corpus and some gensim word vectors and returns a matrix with the average embedding of each sentence\n",
        "  embeddings = []\n",
        "\n",
        "  for sentence in corpus:\n",
        "\n",
        "    embeddings.append(...)\n",
        "\n",
        "  return np.stack(embeddings)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgaTMEKGWrbM"
      },
      "source": [
        "X_pos = create_embeddings(positives, model.wv)\n",
        "X_neg = create_embeddings(negatives, model.wv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdZ0ZmRcV57X",
        "outputId": "df9380fa-f225-4d14-a6ac-d02385fb588b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(X_pos.shape)\n",
        "print(X_neg.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(12500, 50)\n",
            "(12500, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JW6OYaCJE9sg"
      },
      "source": [
        "n_pos = X_pos.shape[0]\n",
        "n_neg = X_neg.shape[0]\n",
        "\n",
        "X = np.concatenate([X_pos, X_neg])\n",
        "y = np.zeros((n_pos + n_neg, ), dtype=int)\n",
        "\n",
        "y[:n_pos] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bN9iurlaYyD8"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOFyKYOgZLyd"
      },
      "source": [
        "#TODO: Sklearn linear classifier\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "linear_model = LinearSVC()\n",
        "linear_model.fit(X_train, y_train)\n",
        "\n",
        "print(linear_model.score(X_train, y_train))\n",
        "print(linear_model.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1BF0tdQLr-k"
      },
      "source": [
        "#TODO: find the best regularization constant C of the linear SVM when you train with only n_samp samples\n",
        "\n",
        "n_samp = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8HjWazHE9WK"
      },
      "source": [
        "# Language modeling\n",
        "\n",
        "Language modeling is a way to train models to generate text. \n",
        "We will see how to use Pytorch models to do language modeling.\n",
        "\n",
        "First, let's look at a few steps that are necessary for text processing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pB2wTS8MTEd"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "Tokenization creates a dictionary that contains all words, and creates an index for each word. \n",
        "Look at the class Dictionary and Corpus below. \n",
        "What do they do ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSsiYLO4MMBo"
      },
      "source": [
        "import os\n",
        "from io import open\n",
        "import torch\n",
        "\n",
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = []\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)\n",
        "\n",
        "\n",
        "class Corpus(object):\n",
        "    def __init__(self, path):\n",
        "        self.dictionary = Dictionary()\n",
        "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
        "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
        "        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
        "\n",
        "    def tokenize(self, path):\n",
        "        \"\"\"Tokenizes a text file.\"\"\"\n",
        "        print(path)\n",
        "        assert os.path.exists(path)\n",
        "        # Add words to the dictionary\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                for word in words:\n",
        "                    self.dictionary.add_word(word)\n",
        "\n",
        "        # Tokenize file content\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            idss = []\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                ids = []\n",
        "                for word in words:\n",
        "                    ids.append(self.dictionary.word2idx[word])\n",
        "                idss.append(torch.tensor(ids).type(torch.int64))\n",
        "            ids = torch.cat(idss)\n",
        "\n",
        "        return ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyRPh4tQIbu5"
      },
      "source": [
        "## RNN and LSTM models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYGnegQF6uqH"
      },
      "source": [
        "We define a class for our RNN model. Fill the \"...\" in the __init__ and forward method, using the Pytorch documentation of nn.RNN and nn.LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgV5NqRVNbcC"
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class RNNModel(nn.Module):\n",
        "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
        "\n",
        "    def __init__(self, rnn_type, num_token, embedding_dim, hidden_dim, num_layers, dropout=0.5):\n",
        "        super(RNNModel, self).__init__()\n",
        "\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.encoder = nn.Embedding(num_token, embedding_dim)\n",
        "\n",
        "        if rnn_type == 'LSTM':\n",
        "            self.rnn = ...\n",
        "        elif rnn_type == 'RNN':\n",
        "            self.rnn = ...\n",
        "        else:\n",
        "            raise NotImplementedError(\"\"\"Only RNN and LSTM are implemented yet\"\"\")\n",
        "            \n",
        "        self.decoder = nn.Linear(hidden_dim, num_token)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "        self.rnn_type = rnn_type\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        emb = self.drop(self.encoder(input))\n",
        "\n",
        "        output, hidden = ...\n",
        "\n",
        "        output = self.drop(output)\n",
        "\n",
        "        decoded = ...\n",
        "\n",
        "        return decoded, hidden\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters())\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            return (weight.new_zeros(self.num_layers, bsz, self.hidden_dim),\n",
        "                    weight.new_zeros(self.num_layers, bsz, self.hidden_dim))\n",
        "        else:\n",
        "            return weight.new_zeros(self.num_layers, bsz, self.hidden_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1_3nHM8PdsC"
      },
      "source": [
        "## Train your own language model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHk1kBdr666J"
      },
      "source": [
        "We are all set to train our language model! Let's get some data first"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUx-TX8NGwAV",
        "outputId": "cedf04f0-fb50-40c4-e088-53f9a8bf74ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget -O train.txt https://raw.githubusercontent.com/pytorch/examples/main/word_language_model/data/wikitext-2/train.txt\n",
        "!wget -O valid.txt https://raw.githubusercontent.com/pytorch/examples/main/word_language_model/data/wikitext-2/valid.txt\n",
        "!wget -O test.txt https://raw.githubusercontent.com/pytorch/examples/main/word_language_model/data/wikitext-2/test.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-16 18:10:15--  https://raw.githubusercontent.com/pytorch/examples/main/word_language_model/data/wikitext-2/train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10797148 (10M) [text/plain]\n",
            "Saving to: ‘train.txt’\n",
            "\n",
            "\rtrain.txt             0%[                    ]       0  --.-KB/s               \rtrain.txt           100%[===================>]  10.30M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2022-03-16 18:10:15 (170 MB/s) - ‘train.txt’ saved [10797148/10797148]\n",
            "\n",
            "--2022-03-16 18:10:15--  https://raw.githubusercontent.com/pytorch/examples/main/word_language_model/data/wikitext-2/valid.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1121681 (1.1M) [text/plain]\n",
            "Saving to: ‘valid.txt’\n",
            "\n",
            "valid.txt           100%[===================>]   1.07M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2022-03-16 18:10:16 (45.5 MB/s) - ‘valid.txt’ saved [1121681/1121681]\n",
            "\n",
            "--2022-03-16 18:10:16--  https://raw.githubusercontent.com/pytorch/examples/main/word_language_model/data/wikitext-2/test.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1256449 (1.2M) [text/plain]\n",
            "Saving to: ‘test.txt’\n",
            "\n",
            "test.txt            100%[===================>]   1.20M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2022-03-16 18:10:16 (50.9 MB/s) - ‘test.txt’ saved [1256449/1256449]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvajwyRI7A6p"
      },
      "source": [
        "We define the arguments that we will need to train our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFACJSMwDOnY"
      },
      "source": [
        "import argparse\n",
        "import time\n",
        "import math\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.onnx\n",
        "\n",
        "args = argparse.Namespace(\n",
        "  data='.',\n",
        "  model='LSTM',\n",
        "  emsize=200,\n",
        "  nhid=200,\n",
        "  nlayers=2,\n",
        "  lr=20,\n",
        "  clip=0.25,\n",
        "  epochs=20,\n",
        "  batch_size=20,\n",
        "  bptt=35,\n",
        "  dropout=0.2,\n",
        "  seed=1111,\n",
        "  cuda=True,\n",
        "  log_interval=200,\n",
        "  save='model.pt'\n",
        ")\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "if args.cuda:\n",
        "    device = \"cuda:0\"\n",
        "else:\n",
        "    device = \"cpu\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uphdpbvu7ILX"
      },
      "source": [
        "Look at the function batchify(), what does it do ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyXPixLIHcDP",
        "outputId": "3655601b-aa1c-4171-e44e-97ef965b44f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "###############################################################################\n",
        "# Load data\n",
        "###############################################################################\n",
        "\n",
        "corpus = Corpus(args.data)\n",
        "\n",
        "# Starting from sequential data, batchify arranges the dataset into columns.\n",
        "# For instance, with the alphabet as the sequence and batch size 4, we'd get\n",
        "# ┌ a g m s ┐\n",
        "# │ b h n t │\n",
        "# │ c i o u │\n",
        "# │ d j p v │\n",
        "# │ e k q w │\n",
        "# └ f l r x ┘.\n",
        "# These columns are treated as independent by the model, which means that the\n",
        "# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient\n",
        "# batch processing.\n",
        "\n",
        "def batchify(data, bsz):\n",
        "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(corpus.train, args.batch_size)\n",
        "val_data = batchify(corpus.valid, eval_batch_size)\n",
        "test_data = batchify(corpus.test, eval_batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./train.txt\n",
            "./valid.txt\n",
            "./test.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xf6uwjnrfHpt"
      },
      "source": [
        "Build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ns9nVCoBfE26"
      },
      "source": [
        "ntokens = len(corpus.dictionary)\n",
        "model = RNNModel(args.model, ntokens, args.emsize, args.nhid, args.nlayers, args.dropout).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovIf9ddsfJWN"
      },
      "source": [
        "Fill up the todos in the train() function to forward the current batch in the model, compute the loss and the gradient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrRaAA8YIMZS"
      },
      "source": [
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "\n",
        "# get_batch subdivides the source data into chunks of length args.bptt.\n",
        "# If source is equal to the example output of the batchify function, with\n",
        "# a bptt-limit of 2, we'd get the following two Variables for i = 0:\n",
        "# ┌ a g m s ┐ ┌ b h n t ┐\n",
        "# └ b h n t ┘ └ c i o u ┘\n",
        "# Note that despite the name of the function, the subdivison of data is not\n",
        "# done along the batch dimension (i.e. dimension 1), since that was handled\n",
        "# by the batchify function. The chunks are along dimension 0, corresponding\n",
        "# to the seq_len dimension in the LSTM.\n",
        "\n",
        "def get_batch(source, i):\n",
        "    seq_len = min(args.bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target\n",
        "\n",
        "\n",
        "def evaluate(model, data_source):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, args.bptt):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            output, hidden = model(data, hidden)\n",
        "            hidden = repackage_hidden(hidden)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
        "\n",
        "    return total_loss / (len(data_source) - 1)\n",
        "\n",
        "\n",
        "def train():\n",
        "    # Turn on training mode which enables dropout.\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    hidden = model.init_hidden(args.batch_size)\n",
        "\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, args.bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "        model.zero_grad()\n",
        "        hidden = repackage_hidden(hidden)\n",
        "\n",
        "        #TODO: forward through the model and compute the loss and the gradients\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
        "        with torch.no_grad():\n",
        "          for p in model.parameters():\n",
        "              p.add_(-lr, p.grad)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch % args.log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / args.log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
        "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                epoch, batch, len(train_data) // args.bptt, lr,\n",
        "                elapsed * 1000 / args.log_interval, cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHqEhT9Bfgop"
      },
      "source": [
        "### Training loop\n",
        "\n",
        "Training for 20 epochs should take ~15 minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CejFqyHbHYdO",
        "outputId": "0789162d-d3ca-46c8-a44c-04e802e9e828",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "source": [
        "# Loop over epochs.\n",
        "lr = args.lr\n",
        "best_val_loss = None\n",
        "\n",
        "# At any point you can hit Ctrl + C to break out of training early.\n",
        "try:\n",
        "    for epoch in range(1, args.epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        train()\n",
        "        val_loss = evaluate(val_data)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                           val_loss, math.exp(val_loss)))\n",
        "        print('-' * 89)\n",
        "        # Save the model if the validation loss is the best we've seen so far.\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            with open(args.save, 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "        else:\n",
        "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "            lr /= 4.0\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-7448f7a43291>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mepoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m89\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-ccf48d9dd25d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'data'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUbNa3uPJRsq",
        "outputId": "50023101-4eb3-4223-94cb-927196884d1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=========================================================================================\n",
            "| End of training | test loss  4.68 | test ppl   107.28\n",
            "=========================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MBsi_IpPiTr"
      },
      "source": [
        "## Generate sentences with a trained language model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5L3kuXeePksh",
        "outputId": "96ddfc73-feb8-4117-bc9e-d83b904b62c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "args.words = 200\n",
        "args.temperature = 1\n",
        "\n",
        "model.eval()\n",
        "\n",
        "ntokens = len(corpus.dictionary)\n",
        "\n",
        "hidden = model.init_hidden(1)\n",
        "input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
        "\n",
        "text = []\n",
        "with torch.no_grad():  # no tracking history\n",
        "    for i in range(args.words):\n",
        "        output, hidden = model(input, hidden)\n",
        "        word_weights = output.squeeze().div(args.temperature).exp().cpu()\n",
        "        word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "        input.fill_(word_idx)\n",
        "\n",
        "        word = corpus.dictionary.idx2word[word_idx]\n",
        "        text.append(word)\n",
        "\n",
        "        if i % args.log_interval == 0:\n",
        "            print('| Generated {}/{} words'.format(i, args.words))\n",
        "\n",
        "print(\" \".join(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Generated 0/200 words\n",
            "Man . In 1867 , Fu Riata recorded the most archaeological civic battery of the series . As its system to the United States was exhausted , the Soviet Union was financed because of a duty as attention to the world ; he gave several new works portraying her house and the details involved in an existing theatre . The city was a <unk> @-@ to @-@ answer industrial network three hours before Comair 's arrival in 2005 . In Mogadishu , Howard he worked The conclave of Germany 4 months after his death in 1975 . The county also began dating to Bristol Gardens to Los Angeles in 2011 , taking out in 1859 . In August 2014 , an article for the O Tempo <unk> was led to Eugène Monsen , William A. Humboldt and <unk> Gurion Street , based on his debut and then set up in 2010 . Shortly sales made to <unk> all of the popular <unk> drinking mechanics , and later in its first year of dense the cause of testing , Chulachomklao individuals match despite the <unk> of an million people to make the right and ominous strike . However , some players\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vicSvGicGcw"
      },
      "source": [
        "## Demo of the best language models\n",
        "\n",
        "https://transformer.huggingface.co/"
      ]
    }
  ]
}